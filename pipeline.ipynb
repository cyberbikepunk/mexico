{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mexican federal budget pre-processing pipeline\n",
    "\n",
    "## Instructions\n",
    "\n",
    "To you run the notebook:\n",
    "\n",
    "1. choose a unique `ITERATION_LABEL` for each pipeline run\n",
    "2. specify and describe your input files (`INPUT_FILES`)\n",
    "3. make sure your column mapping (`COLUMN_ALIASES`) is correct\n",
    "3. run the whole notebook by clicking on __Kernel > Restart & Run All__\n",
    "\n",
    "## Settings\n",
    "\n",
    "Choose a unique iteration label for each pipeline run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ITERATION_LABEL = 'iteration-before-holiday'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put your input files inside the `pipeline.in` folder and describe them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "INPUT_FILES = {\n",
    "    2010: {'name': 'Cuenta_Publica_2010.csv', 'encoding': 'windows-1252'},\n",
    "    2011: {'name': 'Cuenta_Publica_2011.csv', 'encoding': 'windows-1252'},\n",
    "    2012: {'name': 'Cuenta_Publica_2012.csv', 'encoding': 'windows-1252'},\n",
    "    2013: {'name': 'Cuenta_Publica_2013.csv', 'encoding': 'windows-1252'},\n",
    "    2014: {'name': 'Cuenta_Publica_2014.csv', 'encoding': 'windows-1252'},\n",
    "    2015: {'name': 'Cuenta_Publica_2015.csv', 'encoding': 'windows-1252'},\n",
    "    2016: {'name': 'PEF2016_AC01.csv', 'encoding': 'cp850'}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your input files don't all have the same column names, define your mapping here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "COLUMN_ALIASES = {\n",
    "    'Actividad Institucional': ['AI'],\n",
    "    'Adefas': ['ADEFAS'],\n",
    "    'Aprobado': [\n",
    "        'PEF_2016',\n",
    "        'Importe Presupuesto de Egresos de la Federación',\n",
    "        'Importe Presupuesto de Egresos de la Federación (PEF)'\n",
    "    ],\n",
    "    'Ciclo': None,\n",
    "    'Clave de cartera': ['CLAVE_CARTERA'],\n",
    "    'Descripción de Fuente de Financiamiento': ['FUENTE_FINAN_DESCRIPCION'],\n",
    "    'Descripción de Función': ['FUNCIONL_DESCRIPCION'],\n",
    "    'Descripción de Grupo Funcional': [\n",
    "        'Descripción de Finalidad',\n",
    "        'GRUPO_FUN_DESCRIPCION',\n",
    "        'Descripción de Grupo Funcional'\n",
    "    ],\n",
    "    'Descripción de Objeto del Gasto': ['CONCEPTO_DESCRIPCION'],\n",
    "    'Descripción de Programa Presupuestario': ['PROGR_PRES_DESCRIPCION'],\n",
    "    'Descripción de Ramo': ['RAMO_DESCRIPCION'],\n",
    "    'Descripción de Reasignacion': ['REASIGNACION_DESCRIPCION'],\n",
    "    'Descripción de Subfunción': ['SUBFUNCIONL_DESCRIPCION'],\n",
    "    'Descripción de Tipo de Gasto': ['TIPO_GASTO_DESCRIPCION'],\n",
    "    'Descripción de Unidad Responsable': ['UNIDAD_DESCRIPCION'],\n",
    "    'Descripción de la Actividad Institucional': [\n",
    "        'ACTIVIDAD_INST_DESCRIPCION',\n",
    "        'Descripción de Actividad Institucional'\n",
    "    ],\n",
    "    'Descripción de la entidad federativa': ['ENTIDAD_FED_DESCRIPCION'],\n",
    "    'Descripción de la modalidad del programa presupuestario': [\n",
    "        'MODALIDAD_DESCRIPCION',\n",
    "        'Descripción del Identificador del Programa Presupuestario',\n",
    "        'Descripción del Identificador de Programa Presupuestario'\n",
    "    ],\n",
    "    'Devengado': None,\n",
    "    'Ejercicio': None,\n",
    "    'Ejercido': None,\n",
    "    'Entidad Federativa': ['EF'],\n",
    "    'Fuente de Financiamiento': ['FF'],\n",
    "    'Función': ['FN'],\n",
    "    'Grupo Funcional': [\n",
    "        'Finalidad', 'GF', 'Grupo Funcional'\n",
    "    ],\n",
    "    'Modalidad del Programa presupuestario': [\n",
    "        'MOD',\n",
    "        'Identificador de Programa Presupuestario',\n",
    "        'Identificador del Programa Presupuestario'\n",
    "    ],\n",
    "    'Modificado': None,\n",
    "    'Objeto del Gasto': ['CONCEPTO'],\n",
    "    'Pagado': None,\n",
    "    'Programa Presupuestario': ['PP'],\n",
    "    'Ramo': None,\n",
    "    'Reasignacion': ['RA'],\n",
    "    'Subfunción': ['SF'],\n",
    "    'Tipo de Gasto': ['TG'],\n",
    "    'Unidad Responsable': ['UNIDAD']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it. Now just run the notebook from beginning to end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sys import stdout\n",
    "from pandas import read_csv, concat, DataFrame\n",
    "from numpy import nan\n",
    "from os.path import join, isdir\n",
    "from os import mkdir\n",
    "from json import dumps\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BASENAME = 'mexican_federal_budget'\n",
    "INPUT_FOLDER = 'pipeline.in'\n",
    "OUTPUT_FOLDER = 'pipeline.out'\n",
    "ITERATION_FOLDER = join(OUTPUT_FOLDER, ITERATION_LABEL)\n",
    "MERGED_FILE = join(ITERATION_FOLDER, BASENAME + '.merged.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if isdir(ITERATION_FOLDER):\n",
    "    raise ValueError('Please enter a unique iteration label')\n",
    "    \n",
    "mkdir(ITERATION_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding inspection\n",
    "\n",
    "Detect the file encodings of the input files using the `cChardet` utility library. __Warning:__ it's not always accurate. This is meant only as an indication only. In the end, encodings will be taken from `INPUT_FILES`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010 Inspected Cuenta_Publica_2010.csv {'encoding': 'WINDOWS-1252', 'confidence': 0.9900000095367432}\n",
      "2011 Inspected Cuenta_Publica_2011.csv {'encoding': 'WINDOWS-1252', 'confidence': 0.9900000095367432}\n",
      "2012 Inspected Cuenta_Publica_2012.csv {'encoding': 'WINDOWS-1252', 'confidence': 0.9900000095367432}\n",
      "2013 Inspected Cuenta_Publica_2013.csv {'encoding': 'WINDOWS-1252', 'confidence': 0.9900000095367432}\n",
      "2014 Inspected Cuenta_Publica_2014.csv {'encoding': 'WINDOWS-1252', 'confidence': 0.9900000095367432}\n",
      "2015 Inspected Cuenta_Publica_2015.csv {'encoding': 'WINDOWS-1252', 'confidence': 0.9900000095367432}\n",
      "2016 Inspected PEF2016_AC01.csv {'encoding': 'WINDOWS-1252', 'confidence': 0.9900000095367432}\n",
      "\n",
      "Saved encoding detection report to pipeline.out/iteration-before-holiday/encodings.detected.json\n"
     ]
    }
   ],
   "source": [
    "def detect_encodings():\n",
    "    \"\"\"Detect CSV file encoding with the cChardet library\"\"\"\n",
    "\n",
    "    try:\n",
    "        import cchardet as chardet\n",
    "    except ImportError:\n",
    "        cChardet = 'https://github.com/PyYoshi/cChardet'\n",
    "        print('Encoding inspection skipped: install %s', cChardet)\n",
    "        return\n",
    "\n",
    "    results = {}\n",
    "    results_file = join(OUTPUT_FOLDER, ITERATION_LABEL, 'encodings.detected.json')\n",
    "    \n",
    "    for year, file in sorted(INPUT_FILES.items()):\n",
    "        datafile = join(INPUT_FOLDER, file['name'])\n",
    "        \n",
    "        with open(datafile, 'rb') as f:\n",
    "            text = f.read()\n",
    "            \n",
    "        result = chardet.detect(text)\n",
    "        results.update({year: result})\n",
    "        print(year, 'Inspected', file['name'], result)\n",
    "    \n",
    "    with open(results_file, 'w+') as json:\n",
    "        json.write(dumps(results))\n",
    "        print('\\nSaved encoding detection report to', results_file)\n",
    "        \n",
    "detect_encodings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_columns(file, encoding):\n",
    "    \"\"\"Return clean CSV file headers\"\"\"\n",
    "    \n",
    "    with open(file, encoding=encoding) as csv:\n",
    "        header = csv.readline()\n",
    "        return header.replace('\\n', '').split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def force_strings(columns):\n",
    "    \"\"\"Return string enforcement for each column of a CSV file\"\"\"\n",
    "    \n",
    "    for column in columns:\n",
    "        yield column, str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_csv_files():\n",
    "    \"\"\"Load raw data (CSV) files\"\"\"\n",
    "    \n",
    "    batch = {}\n",
    "    \n",
    "    for year, file in sorted(INPUT_FILES.items()):\n",
    "        filepath = join(INPUT_FOLDER, file['name'])\n",
    "        column_names = read_columns(filepath, file['encoding'])\n",
    "        column_types = dict(force_strings(column_names))\n",
    "        \n",
    "        batch[year] = read_csv(filepath, encoding=file['encoding'], dtype=column_types)\n",
    "        print('Loaded', file['name'], 'with encoding', file['encoding'])\n",
    "        stdout.flush()\n",
    "            \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def strip_cell_padding(batch):\n",
    "    for year in sorted(batch.keys()):\n",
    "        for column in batch[year].columns:\n",
    "            batch[year].rename(columns={column: column.strip()}, inplace=True)\n",
    "            batch[year][column] = batch[year][column].apply(lambda x: x.strip() if x is not nan else x)\n",
    "        print(year, 'stripped cell paddings')\n",
    "        stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def delete_empty_columns(batch):\n",
    "    for year in batch.keys():\n",
    "        for column in batch[year].columns:\n",
    "            if 'Unnamed:' in column:\n",
    "                try:\n",
    "                    del batch[year][column]\n",
    "                    print(year, column, 'deleted')\n",
    "                    stdout.flush()\n",
    "                except KeyError:\n",
    "                    pass  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_missing_values(batch):\n",
    "    table = []\n",
    "\n",
    "    for column in get_union_of_columns(batch):\n",
    "        row = {'Column': column}\n",
    "        \n",
    "        for year in batch.keys():\n",
    "            if column in batch[year].columns:\n",
    "                nb_empty_cells = batch[year][column].apply(lambda x: 1 if x is nan else 0).sum()\n",
    "            else:\n",
    "                nb_empty_cells = nan\n",
    "                \n",
    "            row.update({year: nb_empty_cells})\n",
    "            if nb_empty_cells not in (nan, 0):\n",
    "                print(year, 'found', nb_empty_cells, 'missing values in', column)\n",
    "\n",
    "        table.append(row)\n",
    "        \n",
    "    ordered_columns = ['Column']\n",
    "    ordered_columns.extend(sorted(batch.keys()))\n",
    "    \n",
    "    return DataFrame(table).reindex_axis(ordered_columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_duplicates(batch):\n",
    "    for year, df in sorted(batch.items()):\n",
    "        nb_duplicate_lines = df.duplicated().apply(lambda x: 1 if x is True else 0).sum()\n",
    "        print(year, 'found', nb_duplicate_lines, 'duplicate lines')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alias column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_union_of_columns(batch):\n",
    "    union = set()\n",
    "    for year in batch.keys():\n",
    "        union = union | set(batch[year].columns)\n",
    "    return union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from yaml import load\n",
    "\n",
    "def load_aliases(file):\n",
    "    with open(file) as yaml:\n",
    "        aliases = load(yaml.read())\n",
    "        return aliases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def map_columns_to_aliases(batch, list_of_aliases):\n",
    "    for year in sorted(batch.keys()):\n",
    "        for column in sorted(batch[year].columns):\n",
    "            if not column in list_of_aliases:\n",
    "                for reference, aliases in list_of_aliases.items():\n",
    "                    if aliases:\n",
    "                        if column in aliases:\n",
    "                            batch[year].rename(columns={column: reference}, inplace=True)\n",
    "                            print(year, column, 'replaced with', reference)\n",
    "                            stdout.flush()\n",
    "                            break  \n",
    "                else:\n",
    "                    print(year, 'NO ALIAS: ', column)\n",
    "                    stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_overview(batch):\n",
    "    table = []\n",
    "    \n",
    "    for column in get_union_of_columns(batch):\n",
    "        row = {'Column': column}\n",
    "        for year in batch.keys():\n",
    "            row.update({year: column in batch[year].columns})\n",
    "        table.append(row)\n",
    "        \n",
    "    ordered_columns = ['Column']\n",
    "    ordered_columns.extend(sorted(batch.keys()))\n",
    "    \n",
    "    overview = DataFrame(table).reindex_axis(ordered_columns, axis=1)\n",
    "    return overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check expenditure sums\n",
    "\n",
    "There's a little cleaning to do on the amount columns (zeros represented by a dash). Assume thousands are seperated by a comma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "EXPENDITURE_COLUMNS = [\n",
    "    'Ejercido', \n",
    "    'Devengado', \n",
    "    'Aprobado', \n",
    "    'Pagado', \n",
    "    'Modificado', \n",
    "    'Adefas', \n",
    "    'Ejercicio'\n",
    "]\n",
    "\n",
    "def clean_expenditure_columns(batch):\n",
    "    check_sums = []\n",
    "\n",
    "    for column in EXPENDITURE_COLUMNS:\n",
    "        row = {'Column': column}\n",
    "        \n",
    "        for year in sorted(batch.keys()):\n",
    "            try:\n",
    "                series = batch[year][column]\n",
    "                \n",
    "                # I'm assuming -' represents zero\n",
    "                series = series.apply(lambda x: '0' if x == '-' else x)\n",
    "                series = series.apply(lambda x: x.replace(',', '') if x is not nan else x)                \n",
    "                batch[year][column] = series.astype(float)\n",
    "                check_sum = batch[year][column].sum()\n",
    "                \n",
    "                print(year, 'cleaned and summed', column)\n",
    "                \n",
    "            except KeyError:\n",
    "                check_sum = nan\n",
    "                \n",
    "            row.update({year: check_sum})\n",
    "        \n",
    "        check_sums.append(row)\n",
    "\n",
    "    ordered_columns = ['Column']\n",
    "    ordered_columns.extend(sorted(batch.keys()))\n",
    "    return DataFrame(check_sums).reindex_axis(ordered_columns, axis=1)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def do_pipeline():\n",
    "\n",
    "    def echo_section(section):\n",
    "        print('\\n', section, '\\n')\n",
    "\n",
    "    echo_section('Loading files')\n",
    "    datasets = load_csv_files()\n",
    "    \n",
    "    echo_section('Delete empty columns')\n",
    "    delete_empty_columns(datasets)\n",
    "\n",
    "    echo_section('Stripping padding from cells')\n",
    "    strip_cell_padding(datasets)\n",
    "    \n",
    "    echo_section('Counting duplicate lines (NOT de-duplicating)')\n",
    "    count_duplicates(datasets)\n",
    "    \n",
    "    echo_section('Mapping column to aliases')\n",
    "    map_columns_to_aliases(datasets, COLUMN_ALIASES)\n",
    "\n",
    "    echo_section('Counting missing values')\n",
    "    missing_values_report = count_missing_values(datasets)\n",
    "    \n",
    "    echo_section('Building column mapping overview')\n",
    "    column_mapping_report = build_overview(datasets)\n",
    "    \n",
    "    echo_section('Cleaning expenditure columns')\n",
    "    sums_report = clean_expenditure_columns(datasets)\n",
    "    \n",
    "    echo_section('Merging datasets')\n",
    "    merged_dataset = concat(list(datasets.values()))\n",
    "    \n",
    "    missing_file = join(ITERATION_FOLDER, BASENAME + '.missing.tsv')\n",
    "    columns_file = join(ITERATION_FOLDER, BASENAME + '.columns.tsv')\n",
    "    sums_file = join(ITERATION_FOLDER, BASENAME + '.sums.tsv')\n",
    "    aliases_file = join(ITERATION_FOLDER, BASENAME + '.aliases.json')\n",
    "    inputs_file = join(ITERATION_FOLDER, BASENAME + '.inputs.json')\n",
    "\n",
    "    merged_dataset.to_csv(MERGED_FILE, encoding='utf-8', index=False)\n",
    "    missing_values_report.to_csv(missing_file, encoding='utf-8', index=False, sep='\\t')\n",
    "    column_mapping_report.to_csv(columns_file, encoding='utf-8', index=False, sep='\\t')\n",
    "    sums_report.to_csv(sums_file, encoding='utf-8', index=False, sep='\\t')\n",
    "    \n",
    "    with open(aliases_file, 'w+') as json:\n",
    "        json.write(dumps(COLUMN_ALIASES, indent=4))\n",
    "        \n",
    "    with open(inputs_file, 'w+') as json:\n",
    "        json.write(dumps(INPUT_FILES, indent=4))\n",
    "    \n",
    "    print('Saved merged datasets to', MERGED_FILE)    \n",
    "    print('Saved input configuration to', inputs_file)    \n",
    "    print('Saved aliases configuration to', aliases_file)    \n",
    "    print('Saved missing values report to', missing_file)    \n",
    "    print('Saved column mapping report to', columns_file)    \n",
    "    print('Saved check sums report to', sums_file)    \n",
    "\n",
    "    echo_section('Pipeline run \"%s\" done and saved to %s' % (ITERATION_LABEL, ITERATION_FOLDER))\n",
    "\n",
    "    return merged_dataset, column_mapping_report, missing_values_report, sums_report, datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Loading files \n",
      "\n",
      "Loaded Cuenta_Publica_2010.csv with encoding windows-1252\n",
      "Loaded Cuenta_Publica_2011.csv with encoding windows-1252\n",
      "Loaded Cuenta_Publica_2012.csv with encoding windows-1252\n",
      "Loaded Cuenta_Publica_2013.csv with encoding windows-1252\n",
      "Loaded Cuenta_Publica_2014.csv with encoding windows-1252\n",
      "Loaded Cuenta_Publica_2015.csv with encoding windows-1252\n",
      "Loaded PEF2016_AC01.csv with encoding cp850\n",
      "\n",
      " Delete empty columns \n",
      "\n",
      "2011 Unnamed: 25 deleted\n",
      "2011 Unnamed: 26 deleted\n",
      "2011 Unnamed: 27 deleted\n",
      "2011 Unnamed: 28 deleted\n",
      "2011 Unnamed: 29 deleted\n",
      "2011 Unnamed: 30 deleted\n",
      "2011 Unnamed: 31 deleted\n",
      "2011 Unnamed: 32 deleted\n",
      "2011 Unnamed: 33 deleted\n",
      "2011 Unnamed: 34 deleted\n",
      "2011 Unnamed: 35 deleted\n",
      "2011 Unnamed: 36 deleted\n",
      "2011 Unnamed: 37 deleted\n",
      "2011 Unnamed: 38 deleted\n",
      "2011 Unnamed: 39 deleted\n",
      "2011 Unnamed: 40 deleted\n",
      "2011 Unnamed: 41 deleted\n",
      "\n",
      " Stripping padding from cells \n",
      "\n",
      "2010 stripped cell paddings\n",
      "2011 stripped cell paddings\n",
      "2012 stripped cell paddings\n",
      "2013 stripped cell paddings\n",
      "2014 stripped cell paddings\n",
      "2015 stripped cell paddings\n",
      "2016 stripped cell paddings\n",
      "\n",
      " Counting duplicate lines (NOT de-duplicating) \n",
      "\n",
      "2010 found 34 duplicate lines\n",
      "2011 found 37 duplicate lines\n",
      "2012 found 560 duplicate lines\n",
      "2013 found 0 duplicate lines\n",
      "2014 found 47 duplicate lines\n",
      "2015 found 16 duplicate lines\n",
      "2016 found 0 duplicate lines\n",
      "\n",
      " Mapping column to aliases \n",
      "\n",
      "2010 Descripción de Finalidad replaced with Descripción de Grupo Funcional\n",
      "2010 Finalidad replaced with Grupo Funcional\n",
      "2011 Descripción de Finalidad replaced with Descripción de Grupo Funcional\n",
      "2011 Finalidad replaced with Grupo Funcional\n",
      "2012 Descripción de Finalidad replaced with Descripción de Grupo Funcional\n",
      "2012 Finalidad replaced with Grupo Funcional\n",
      "2013 Descripción de Finalidad replaced with Descripción de Grupo Funcional\n",
      "2013 Finalidad replaced with Grupo Funcional\n",
      "2014 ADEFAS replaced with Adefas\n",
      "2016 ACTIVIDAD_INST_DESCRIPCION replaced with Descripción de la Actividad Institucional\n",
      "2016 AI replaced with Actividad Institucional\n",
      "2016 CLAVE_CARTERA replaced with Clave de cartera\n",
      "2016 CONCEPTO replaced with Objeto del Gasto\n",
      "2016 CONCEPTO_DESCRIPCION replaced with Descripción de Objeto del Gasto\n",
      "2016 EF replaced with Entidad Federativa\n",
      "2016 ENTIDAD_FED_DESCRIPCION replaced with Descripción de la entidad federativa\n",
      "2016 FF replaced with Fuente de Financiamiento\n",
      "2016 FN replaced with Función\n",
      "2016 FUENTE_FINAN_DESCRIPCION replaced with Descripción de Fuente de Financiamiento\n",
      "2016 FUNCIONL_DESCRIPCION replaced with Descripción de Función\n",
      "2016 GF replaced with Grupo Funcional\n",
      "2016 GRUPO_FUN_DESCRIPCION replaced with Descripción de Grupo Funcional\n",
      "2016 MOD replaced with Modalidad del Programa presupuestario\n",
      "2016 MODALIDAD_DESCRIPCION replaced with Descripción de la modalidad del programa presupuestario\n",
      "2016 PEF_2016 replaced with Aprobado\n",
      "2016 PP replaced with Programa Presupuestario\n",
      "2016 PROGR_PRES_DESCRIPCION replaced with Descripción de Programa Presupuestario\n",
      "2016 RA replaced with Reasignacion\n",
      "2016 RAMO_DESCRIPCION replaced with Descripción de Ramo\n",
      "2016 REASIGNACION_DESCRIPCION replaced with Descripción de Reasignacion\n",
      "2016 SF replaced with Subfunción\n",
      "2016 SUBFUNCIONL_DESCRIPCION replaced with Descripción de Subfunción\n",
      "2016 TG replaced with Tipo de Gasto\n",
      "2016 TIPO_GASTO_DESCRIPCION replaced with Descripción de Tipo de Gasto\n",
      "2016 UNIDAD replaced with Unidad Responsable\n",
      "2016 UNIDAD_DESCRIPCION replaced with Descripción de Unidad Responsable\n",
      "\n",
      " Counting missing values \n",
      "\n",
      "2011 found 1 missing values in Modalidad del Programa presupuestario\n",
      "2011 found 1 missing values in Descripción de Tipo de Gasto\n",
      "2011 found 1 missing values in Descripción de Programa Presupuestario\n",
      "2011 found 1 missing values in Fuente de Financiamiento\n",
      "2011 found 1 missing values in Objeto del Gasto\n",
      "2011 found 1 missing values in Descripción de la Actividad Institucional\n",
      "2011 found 1 missing values in Descripción de Objeto del Gasto\n",
      "2012 found 28 missing values in Descripción de Objeto del Gasto\n",
      "2011 found 1 missing values in Descripción de Fuente de Financiamiento\n",
      "2011 found 1 missing values in Aprobado\n",
      "2012 found 1 missing values in Aprobado\n",
      "2011 found 1 missing values in Descripción de la modalidad del programa presupuestario\n",
      "2011 found 1 missing values in Tipo de Gasto\n",
      "2012 found 172 missing values in Descripción de la entidad federativa\n",
      "2011 found 1 missing values in Ejercido\n",
      "2011 found 1 missing values in Programa Presupuestario\n",
      "\n",
      " Building column mapping overview \n",
      "\n",
      "\n",
      " Cleaning expenditure columns \n",
      "\n",
      "2010 cleaned and summed Ejercido\n",
      "2011 cleaned and summed Ejercido\n",
      "2012 cleaned and summed Ejercido\n",
      "2013 cleaned and summed Ejercido\n",
      "2013 cleaned and summed Devengado\n",
      "2014 cleaned and summed Devengado\n",
      "2015 cleaned and summed Devengado\n",
      "2010 cleaned and summed Aprobado\n",
      "2011 cleaned and summed Aprobado\n",
      "2012 cleaned and summed Aprobado\n",
      "2013 cleaned and summed Aprobado\n",
      "2014 cleaned and summed Aprobado\n",
      "2015 cleaned and summed Aprobado\n",
      "2016 cleaned and summed Aprobado\n",
      "2014 cleaned and summed Pagado\n",
      "2015 cleaned and summed Pagado\n",
      "2014 cleaned and summed Modificado\n",
      "2015 cleaned and summed Modificado\n",
      "2014 cleaned and summed Adefas\n",
      "2015 cleaned and summed Adefas\n",
      "2014 cleaned and summed Ejercicio\n",
      "2015 cleaned and summed Ejercicio\n",
      "\n",
      " Merging datasets \n",
      "\n"
     ]
    }
   ],
   "source": [
    "merged_budget, column_mapping, missing_values, sums, raw_data = do_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list(merged_budget.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged_budget.sample(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "column_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(MERGED_FILE) as file:\n",
    "    for n in range(10):\n",
    "        print(file.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
